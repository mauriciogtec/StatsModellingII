[
["list-2-bayesian-inference-in-gaussian-models.html", "3 List 2: Bayesian Inference in Gaussian Models 3.1 Bayesian inference in a simple Gaussian model", " 3 List 2: Bayesian Inference in Gaussian Models 3.1 Bayesian inference in a simple Gaussian model Let’s start with a simple, one-dimensional Gaussian example, where \\[y_i |\\mu, \\sigma^2 \\sim \\mbox{N}(\\mu,\\sigma^2).\\] We will assume that \\(\\mu\\) and \\(\\sigma\\) are unknown, and will put conjugate priors on them both, so that \\[ \\begin{aligned} \\sigma^2 \\sim&amp; \\mbox{Inv-Gamma}(\\alpha_0, \\beta_0)\\\\ \\mu|\\sigma^2 \\sim&amp; \\mbox{Normal}\\left(\\mu_0, \\frac{\\sigma^2}{\\kappa_0}\\right)\\end{aligned} \\] or, equivalently, \\[ \\begin{aligned} y_i |\\mu, \\omega \\sim&amp; \\mbox{N}(\\mu,1/\\omega)\\\\ \\omega \\sim&amp; \\mbox{Gamma}(\\alpha_0, \\beta_0)\\\\ \\mu|\\omega \\sim&amp; \\mbox{Normal}\\left(\\mu_0, \\frac{1}{\\omega\\kappa_0}\\right)\\end{aligned} \\] We refer to this as a normal/inverse gamma prior on \\(\\mu\\) and \\(\\sigma^2\\) (or a normal/gamma prior on \\(\\mu\\) and \\(\\omega\\)). We will now explore the posterior distributions on \\(\\mu\\) and \\(\\omega\\)(/\\(\\sigma^2\\)) – much of this will involve similar results to those obtained in the first set of exercises. Exercise 3.1 Derive the conditional posterior distributions \\(p(\\mu, \\omega|y_1,\\dots, y_n)\\) (or \\(p(\\mu, \\sigma^2|y_1,\\dots, y_n)\\)) and show that it is in the same family as \\(p(\\mu, \\omega)\\). What are the updated parameters \\(\\alpha_n, \\beta_n,\\mu_n\\) and \\(\\kappa_n\\)? Solution. These computations are very similar to those in the previous list. So here is the summarised version using of the direct computation with Bayes’ rule \\[ \\begin{aligned} p(\\mu, \\omega \\mid y) &amp; \\propto p(y \\mid \\mu, \\omega ) p(\\mu,\\sigma) \\\\ &amp; = p(y\\mid \\mu, \\omega ) p(\\mu \\mid \\omega)p(\\omega) \\\\ &amp; \\propto \\omega^{n/2}\\exp\\left\\{ -\\frac{\\omega}{2} \\sum_{i=1}^n(y_i - \\mu)^2 \\right\\} \\exp\\left\\{ -\\frac{\\omega}{2\\kappa_0} (\\mu - \\mu_0)^2 \\right\\} \\omega^{\\alpha_0-1}\\exp\\{-\\beta_0 \\omega\\} \\\\ &amp; \\propto \\omega^{n/2}\\exp\\left\\{ -\\frac{\\omega}{2}\\left(\\sum_{i=1}^n(y_i - \\bar{y})^2 + n(\\bar{y} - \\mu)^2 \\right)\\right\\} \\\\ &amp; \\hspace{2cm} \\exp\\left\\{ -\\frac{\\omega}{2\\kappa_0} (\\mu - \\mu_0)^2 \\right\\} \\omega^{\\alpha_0-1}\\exp\\{-\\beta_0 \\omega\\} \\\\ &amp; \\propto \\omega^{n/2 + \\alpha_0 - 1}\\exp\\left\\{ -\\frac{\\omega}{2}\\left(\\left(n + \\frac{1}{\\kappa_0}\\right)\\mu^2 - 2\\left(n\\bar{y} + \\frac{\\mu_0}{\\kappa_0}\\right) \\mu + n\\bar{y}^2 + \\frac{\\mu_0^2}{\\kappa_0}\\right) \\right\\} \\\\ &amp; \\hspace{2cm} \\exp\\left\\{ -\\left( \\beta_0 + \\frac{1}{2}\\sum_{i=1}^n(y_i - \\bar{y})^2 \\right)\\omega \\right\\} \\\\ &amp;= \\omega^{n/2 + \\alpha_0 - 1} \\exp\\left\\{ -\\frac{\\omega(n + 1 / \\kappa_0)}{2} \\left(\\mu - \\frac{n \\bar{y} + \\mu_0/\\kappa_0}{n + 1/\\kappa_0}\\right)^2 \\right\\} \\\\ &amp; \\hspace{2cm} \\exp\\left\\{ -\\left( \\beta_0 + \\frac{1}{2}\\sum_{i=1}^n(y_i - \\bar{y})^2 + \\frac{n /\\kappa_0}{n + 1/\\kappa_0}(\\bar{y}-\\mu_0)^2 \\right)\\omega \\right\\} \\end{aligned} \\] Define \\[ \\tau_n = n + 1/\\kappa_0 \\\\ \\mu_n = \\frac{n}{\\tau_n}\\bar{y} + \\frac{1/\\kappa_0}{\\tau_n}\\mu_0 \\\\ \\alpha_n = \\alpha_0+ \\frac{n-1}{2}\\\\ \\beta_n = \\beta_0 + \\frac{1}{2}\\left(\\sum_{i=1}^n (y_i- \\bar{y})^2 + \\frac{n /\\kappa_0}{n + 1/\\kappa_0}(\\bar{y}-\\mu_0)^2 \\right) \\] It follows that \\[ (\\mu, \\omega \\mid y ) \\sim \\mathrm{NormalGamma}\\left(\\mu_n, \\tau_n, \\alpha_n, \\beta_n \\right) \\] Exercise 3.2 Derive the conditional posterior distribution \\(p(\\mu|\\omega,y_1,\\dots, y_n)\\) and \\(p(\\omega|y_1,\\dots, y_n)\\) (or if you’d prefer, \\(p(\\mu|\\sigma^2, y_1,\\dots, y_n)\\) and \\(p(\\sigma^2|y_1,\\dots, y_n)\\)). Based on this and the previous exercise, what are reasonable interpretations for the parameters \\(\\mu_0,\\kappa_0, \\alpha_0\\) and \\(\\beta_0\\)? Solution. From the last expression above it’s very easy to see that \\[ (\\omega \\mid y) \\sim \\mathrm{Gamma}(\\alpha_n, \\beta_n) \\] and \\[ (\\mu \\mid \\omega, y) \\sim \\mathrm{Normal}(\\mu_n, \\omega / \\kappa_n). \\] where \\(\\kappa_n = 1 /\\tau_n\\). Let’s now assume that each \\(y_i\\) is a \\(d\\)-dimensional vector, such that \\[y_i \\sim \\mbox{N}(\\mu, \\Sigma)\\] for \\(d\\)-dimensional mean vector \\(\\mu\\) and \\(d\\times d\\) covariance matrix \\(\\Sigma\\). We will put an prior on \\(\\Sigma\\). The inverse Wishart distribution is a distribution over positive-definite matrices parametrized by \\(\\nu_0&gt;d-1\\) degrees of freedom and positive definite matrix \\(\\Lambda_0^{-1}\\), with pdf \\[p(\\Sigma|\\nu_0, \\Lambda_0^{-1}) = \\frac{|\\Lambda|^{\\nu_0/2}}{2^{(\\nu_0 +d)/2}\\Gamma_d(\\nu_0/2)}|\\Sigma|^{-\\frac{\\nu_0+d+1}{2}}e^{-\\frac{1}{2}\\mbox{tr}(Lambda\\Sigma^{-1})}\\] where \\(\\Gamma_d(x) = \\pi^{d(d-1)/4}\\prod_{i=1}^d\\Gamma\\left(x-\\frac{j-1}{2}\\right)\\). "]
]
