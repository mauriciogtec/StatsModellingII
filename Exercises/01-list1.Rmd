---
title: "Statistical Modelling II"
date: "`r paste('Version:', Sys.Date())`"
author:
  - Content by Sinead Williamson
  - Solutions by Mauricio Garcia Tec  
---


# Preliminaries

## Exchangeability and de Finetti's theorem

A standard situation in statistics is to be presented with a sequence of observations, and use them to make predictions about future observations. In order to do so, we need to make certain assumptions about the nature of the statistical relationships between the sequence of observations.

A common assumption is that our data are **exchangeable**, meaning that their joint probability is invariant under permutations. More concretely, we say a sequence of N observations is finitely exchangeable if 
$$P(X_1\in A_1,X_2\in A_2,\dots, X_N\in A_n) = P(X_{\sigma(1)}\in A_1, X_{\sigma(2)}\in A_2,\dots, X_{\sigma(N)}\in A_n)$$
for any permutation of the integers 1 through $N$, and that an infinite sequence is infinitely exchangeable if this invariance holds for all values of $N$.

```{exercise}
Clearly, all iid sequences are exchangeable, but not all exchangeable sequences are iid. Consider an urn, containing r red balls and b blue balls. A sequence of colors is generated by repeatedly sampling a ball from the urn, noting its color, and then returning the ball, plus another ball of the same color, to the urn. Show that the resulting sequence is exchangeable, but not iid.
```

### De Finneti's theorem


Loosely speaking, de Finetti's Theorem states if a sequence of random variables is infinitely exchangeable, those random variables must be conditionally i.i.d. given some set of parameters. More formally,

```{theorem, name='de Finetti'}
  Let $(X_1,X_2,\dots)$ be an infinite sequence of random variables in some space $\mathcal{X}$. This sequence is infinitely exchangeable if and only if there exists a probability distribution $Q_\theta$, parametrized by some random parameter $\theta\sim \nu$, such that the $X_i$ are conditionally iid given $Q_\theta$ and such that
$$P(X_1\in A_1,X_2\in A_2,\dots) = \int_{\Theta}\prod_{i=1}^\infty Q_\theta(A_i) \nu(d\theta).$$
```

This means we can imagine that any exchangeable sequence has been generated as a sequence of i.i.d.\ random variables with some unknown law. This provides a motivation for Bayesian inference: We have a hierarchical model, where data are generated according to some distribution parametrized by a random (in the Bayesian context -- i.e.\ unknown/uncertain) variable $\theta$, and our uncertainty about $\theta$ is characterized by some distribution $\nu$.

Let's consider the 0/1 form of de Finetti's theorem, for exchangeable sequences of binary variables:


```{theorem, name = 'de Finetti 0/1'}
  An infinite sequence $(X_1,X_2,\dots)$ of binary random variables is exchangeable if and only if its distribution can be written as
  $$\begin{aligned}P(X_1=x_1,X_2=x_2,\dots, X_N=x_N)  =& \int_0^1\prod_{i=1}^N\left\{\theta^{x_i}(1-\theta)^{1-x_i}\right\} d\nu(\theta)\\
    =&\int_0^1 \theta^{k}(1-\theta)^{N-k} d\nu(\theta)\end{aligned}$$
  where $k=\sum_ix_i$.
```

We will now work through (most of) a proof in the next two exercises.

```{exercise}
  We will start off with a finite sequence $(X_1,\dots, X_M)$. For any $N\leq M$, show that
  $$P\left(\sum_{i=1}^N X_i = s\Big|\sum_{i=1}^M X_i = t\right) = \frac{{t\choose s}{M-t\choose N-s}}{{M \choose N}}$$
```

We can therefore write
\begin{equation}
P\left(\sum_{i=1}^N X_i = s\right) = {N\choose s}\sum_{t=s}^{M-N+s}\frac{(t)_s(M-t)_{n-s}}{(M)_N}P\left(\sum_{i=1}^M X_i = t\right),
\label{eqn:a}
\end{equation}
where $(x)_y = x(x-1)\dots (x-y+1)$.

Let $F_M(\theta)$ be the distribution function of $\frac{1}{M}(X_1, + \dots, + X_M)$  --  i.e.\ a step function between 0 and 1, with steps of size $P(\sum_i X_i= t)$ at $t=0,1,\dots, M$. Then we can rewrite Equation~\ref{eqn:a} as

$$P\left(\sum_{i=1}^N X_i = s\right) = {N\choose s} \int_0^1\frac{(M\theta)_s(M(1-\theta))_{N-s}}{(M)_N}dF_M(\theta)$$
```{exercise}
  Show that, as $M\rightarrow \infty$, we can write
  $$P\left(\sum_{i=1}^N X_i = s\right) \rightarrow {N\choose s} \int_0^1\theta^s(1-\theta)^{N-s}dF_M(\theta)$$
```

The proof is completed using a result (the Helly Theorem), that shows that any sequence $\{F_M(\theta); M=1,2,\dot\}$ of probability distributions on [0,1] contains a subsequence that converges to $F(\theta)$.


## Exponential family of distributions


De Finetti's theorem can be seen as a motivation for Bayesian inference. If our data are exchangeable, we know that they are iid according to some unknown probability distribution $F_\theta(X)$, which we can think of as a **likelihood function**, and that they can be represented using an mixture of such iid sequences. As we saw from the 0/1 case, the distribution over probabilities is given by the limit of the empirical distribution function. When not working in this limit, we may choose to model this distribution over the parameters of our likelihood function using a **prior** distribution $\pi(\theta)$ -- ideally one that both assigns probability mass to where we expect the empirical distribution might concentrate, and for which $\int_\Theta F_\theta(X) \pi(d\theta)$ is tractable.

The exponential family of probability distributions is the class of distributions parametrized by $\theta$ whose density can be written as

$$p(x|\theta) = h(x)\exp\{\eta(\theta)^TT(x) - A(\eta(\theta))\}$$
where 

* $\eta(\theta)$ (sometimes just written as $\eta$), is a transformation of $\theta$ that is often referred to as the \textbf{natural or canonical parameter}.
* $T(X)$ is known as a \textbf{sufficient statistic} of $X$. We see that $p(x|\theta)$ depends only on $X$ through $T(X)$, implying that $T(X)$ contains all the relevant information about $X$.
*\item* $A(\eta(\theta)$ (or $A(\eta)$) is known as the \textbf{cumulant function} or the \textbf{log partition function} (remember, a partition function provides a normalizing constant).

```{example, name='The Bernoulli distribution'}
  A Bernoulli random variable $X$ takes the value $X=1$ with probability $\pi$ and $X=0$ with probability $1-\pi$; it's density can be written:

  $$\begin{aligned}
    p(x|\pi) =& \pi^x(1-\pi)^{1-x}\\
    =& \exp\left\{\log\left(\frac{\pi}{1-\pi}\right)x + \log(1-\pi)\right\}\end{aligned}$$
    By rewriting in this exponential family form, we see that
    \begin{itemize}
    \item $\eta = \frac{\pi}{1-\pi}$
    \item $T(x) = x$
    \item $A(\eta) = -\log(1-\pi) = \log(1+e^{\eta})$
    \item $h(x)=1$
    \end{itemize}
```

```{exercise}
  The Poisson random variable has PDF
  $$p(x|\lambda) = \frac{\lambda^xe^{-\lambda}}{x!}$$
  Re-write the density of the Poisson random variable in exponential family form. What are $\eta$, $T(x)$, $A(\eta)$ and $h(x)$? What about if we have $n$ independent samples $x_1,\dots, x_n$?
```

```{exercise}
  The gamma random variable has PDF
  $$p(x|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$$
  What are the natural parameters and sufficient statistics for the gamma distribution, given $n$ observations $x_1,\dots, x_N$?
```


### Cumulants and moments of exponential families

We are probably most familiar with using the PDF or the CDF of a random variable to describe its distribution, but there are other representations that can be useful. The \textbf{moment generating function} $M_X(s) = E[\exp(s^Tx)] = \int_\mathcal{X} e^{s^Tx}p_X(x) dx$ is the Laplace transform of the PDF $p_X(x)$. As the name suggests, we can use the moment-generating function to generate the (uncentered) moments of a random variable; the $n$th moment is given by

$$m_n = \frac{d^nM_X}{ds^n}\Bigr|_{s=0}$$

```{exercise}
  For exponential family random variables, we know that the sufficient statistic $T(X)$ contains all the information about $X$, so (for univariate $X$) we can write the moment generating function of the sufficient statistic as $E[e^{sT(x)}|\eta]$. Show that the moment generating function for the sufficient statistic of  an arbitrary exponential family random variable with natural parameter $\eta$ can be written as
    $$M_{T(X)}(s) = \exp{A(\eta+s) - A(\eta)}$$
```

A related representation is the \textbf{cumulant generating function} $C_X(s) = \log E[e^{s^Tx}] = \log(M_X(s))$. Clearly, for exponential families this takes the form $C_{T(X)}(s) = A(\eta+s)-A(\eta)$. This explains why $A(\eta)$ is sometimes called the cumulant function! The cumulant function can be used to generate the cumulants of a distribution as

$$\kappa_n = \frac{d^nC_X}{ds^n}\Bigr|_{s=0}$$

The first three cumulants are the same as the first three central moments of the distribution -- meaning, the cumulant generative function is a useful tool for calculating mean, variance and the third central moment.

```{exercise}
  It is usually easier to calculate mean and variance using the cumulant generating function rather than the moment generating function. Starting from the exponential family representation of the Poisson distribution from Exercise 1.4, calculate the mean and variance of the Poisson using a) the moment generating function, and b) the cumulant generating function.
```

### Conjugate priors

Exponential families are very important in Bayesian statistics because, for any exponential family likelihood, we can find an conjugate exponential family prior. If our likelihood takes the form

$$f(x|\eta) = h(x)\exp\left\{\eta^TT(x) - A(\eta)\right\}$$

then a conjugate prior is given by

$$p(\eta|\xi,\nu) = g(\xi,\nu)\exp\left\{\eta^T\xi - \nu A(\eta)\right\}$$

Below are some exercises based on common conjugate priors.

```{exercise}
  Suppose we have $N$ independent observations $x_1,\dots, x_N \stackrel{\small{iid}}{\sim}\mbox{Normal}(\mu, \sigma^2)$. If $\sigma^2$ is known and $\mu\sim \mbox{Normal}(\mu_0,\sigma_0^2)$, derive the posterior for $\mu|x_1,\dots, x_N$
```

```{exercise}
  Now, let's assume  $x_1,\dots, x_N \stackrel{\small{iid}}{\sim}\mbox{Normal}(\mu, \sigma^2)$ with known mean $\mu$ but unknown variance $\sigma^2$. Let's express the likelihood in terms of the precision, $\omega=1\sigma^2$:
  $$f(x_i|\mu, \omega) = \sqrt{\frac{\omega}{2\pi}} \exp\left\{-\frac{\omega}{2}(x_i-\mu)^2\right\}$$
  Let $\omega$ have a gamma prior (this is also known as putting an inverse-gamma prior on $\sigma^2$):
  $$p(\omega) = \frac{\beta^\alpha}{\Gamma(\alpha)}\omega^{\alpha-1}e^{-\beta \omega}$$
  Derive the posterior distribution for $\omega$
```

```{exercise}
  Let's assume $x \sim \mbox{Normal}(0, \sigma^2)$ and that $\sigma^2\sim\mbox{InvGamma}(\alpha,\beta)$ (i.e.\ $1/\sigma^2 \sim \mbox{Gamma}(\alpha,\beta)$). Show that the marginal distribution of $x$ is given by a Student's $t$ distribution.
```

## Multariate Normal Distribution

So far, we have looked at univariate random variables - particularly, the univariate normal random variable, which is characterized by its mean and variance. We will often work with the multivariate normal distribution, a natural generalization characterized by a mean vector and a covariance matrix.

```{exercise, name = 'covariance matrix'}
  The covariance matrix $\Sigma$ of a vector-valued random variable $x$ is the matrix whose entries $\Sigma(i,j) = cov(x_i,x_j)$ are given by the covariance between the $i$th and $j$th elements of $x$, giving

  $$\Sigma = E\left[(x-\mu)(x-\mu)^T\right]$$

  Show that a) $\Sigma = E[xx^T] - \mu\mu^T$; b) if the covariance of $x$ is $\sigma$, then the covariance of $Ax+b$ is $A\Sigma A^T$
```

```{exercise, name = 'Standard multivariate normal'}
  The simplest multivariate normal, known as the standard multivariate normal, occurs where the entries of $x$ are independent and have mean 0 and variance 1. a) What is the moment generating function of a univariate normal, with mean $m$ and variance $v^2$? b) Express the PDF and moment generating function of the standard multivariate normal, in vector notation.
```


```{exercise, name = 'Multivariate normal'}
  A random vector $x$ has multivariate normal distribution if and only if every linear combination of its elements is univariate normal, i.e.\ if the scalar value $z = a^Tx$ is normally distributed for all possible $x$. Prove that this implies that $x$ is multivariate normal if and only if its moment generating function takes the form $M_X(s) = \exp\{s^T\mu + s^T\Sigma s\}$, where $\mu$ and $\Sigma$ are the mean and covariance of $x$. \textit{Hint: We know the moment generating function of $z$ in terms of the mean and variance of $z$, from the previous question...}
```
  
```{exercise, name = 'Relationship to standard multivariate normal'}
  An equivalent statement is that a random vector $x$ has multivariate normal distribution if and only if it can be written in the form
  $$x = Dz + \mu$$
  for some matrix $D$, real-valued vector $\mu$, and vector $z$ distributed according to a standard multivariate normal. Express the moment generating function of $x$ in terms of $D$, and uncover the relationship between $D$ and $\Sigma$. Use this result to suggest a method for generating multivariate normal random variables, if you have a method for generating Normal(0,1) univariate random variables.
```

```{exercise}
  Use the result from the previous question to show that the PDF of a multivarite normal random vector $x\sim\mbox{Normal}(\mu, \Sigma)$ takes the form

  $$p(x) = \frac{1}{(2\pi)^{n/2}}|\Sigma|^{-1/2}\exp\left\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right\},$$
by using a change-of-variables from the standard multivariate normal distribution.
```

### Manipulation of multivariate normals

Like its univariate counterpart, the multivariate normal distribution is closed under a number of operations, which we will explore here.

```{exercise, name = 'marginal distribution'}
  Let us assume that $x\sim \mbox{Normal}(\mu, \Sigma)$, and let us partition $x$ into 2 components $x_1$ and $x_2$. Let us similarly partition $\mu$ and $\sigma$ so that

  $$\mu  = (\mu_1, \mu_2)^T \qquad \qquad \Sigma = \begin{pmatrix}\Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}\end{pmatrix} = \begin{pmatrix}\Sigma_{11} & \Sigma_{12} \\ \Sigma_{12}^T & \Sigma_{22}\end{pmatrix}$$

  Derive the marginal distribution of $x_1$.
```

```{exercise, name = 'Precision matrix'}
  Earlier, we chose to express a univariate normal random variable in terms of its precision, to make math easier. We can also express a multivariate normal in terms of a precision matrix $\Omega = \Sigma^{-1}$. Partition $\Omega$ as
  $$\Omega = \begin{pmatrix}\Omega_{11} & \Omega_{12} \\ \Omega_{12}^T & \Omega_{22}\end{pmatrix}$$
  and express $\Omega_{11}$, $\Omega_{12}$ and $\Omega_{22}$ in terms of $\Sigma_{11}$, $\Sigma_{12}$ and $\Sigma_{22}$. \textit{Hint: You'll need the matrix inversion lemma}
```

```{exercise, name = 'Conditional distribution'}
  The conditional distribution of $x_1|x_2$ is also normal, with mean $\mu_1+\Sigma_{12}\sigma_{22}^{-1}(x_2-\mu_2)$ and covariance $\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^T$. Prove this for the case where $\mu$ is zero (the general case isn't really harder, just more tedious). \textit{Hint: ignore any constants that don't involve $x_1$. You might want to work with the log conditional density.}
```

```{exercise, name = 'Conjugacy'}
%  If $x\sim \mbox{Normal}(\mu, \Sigma)$ and $\mu \sim \mbox{Normal}(\mu_0,\Sigma_0)$, derive the posterior of $\mu|x$ \textit{again, ignoring normalizing constancts will make this easier}.
```
  

## Frequentist estimation and uncertainty quantification

In this section, we're going to go over basic frequentist approaches to inference, with a focus on multiple linear regression (since we're next going to look at Bayesian regression). Some of this should be familiar to you, although we will go into quite some depth. Throughout the remainder of this section, we are going to assume our data follow a linear model, of the form
$$y_i = x_i^T\beta + \epsilon_i,\qquad i=1,\dots,N$$

There are a number of options for estimating $\beta$. Three commonly used techniques are:

1. **Method of Moments**: Select $\hat{\beta}$ so that the empirical moments of the observations match the theoretical moments.
2. **Maximum likelihood**: Assume a model for generating the $\epsilon_i$, and find the value of $\hat{\beta}$ that maximizes the likelihood.
3. **Loss function**: Construct a loss function between the $y_i$ and $x_i^T\hat{\beta}$, and minimize that loss function.


```{exercise, name = 'method of moments'}
  To obtain the theoretical moments, we can assume that $E[y_i|x_i] = x_i^T\beta$, implying that the covariance between the predictors $x_i$ and the residuals is zero. By setting the sample covariance between the $x_i$ and the $\epsilon_i$ to zero, derive a method of moments estimator $\hat{\beta}_{MM}$
```

```{exercise, name = 'maximum likelihood'}
  Show that, if we assume $\epsilon_i\sim \mbox{Normal}(0,\sigma^2)$, then the ML estimator $\hat{\beta}_{ML}$ is equivalent to the method of moments estimator.
```



```{exercise, name = 'Least squares loss function'}
  Show that if we assume a quadratic loss function, i.e.\ $\hat{\beta}_{LS} = \arg\min_{\beta\in \mathbb{R}^p}\sum_{i=1}^N(y_i - x_i^T\beta)^2$, we recover the same estimator again.
```

```{exercise, name = 'Ridge regression'}
  We may wish to add a regularization term to our loss term. For example, ridge regression involves adding an L2 penalty term, so that
  $$\hat{\beta}_{\small{ridge}} = \arg\min_{\beta\in \mathbb{R}^p}\sum_{i=1}^N(y_i - x_i^T\beta)^2 \; s.t. \; \sum_{j=1}^p \beta_j^2 \leq t$$
  for some $t\geq 1$.

  Reformulate this constrained optimization using a Lagrange multiplier, and solve to give an expression for $\hat{\beta}_{\small{ridge}}$. Comparing this with the least squares estimator, comment on why this estimator might be prefered in practice.
```

