<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.5.15 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SDS 383d</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path=""><a href="#preliminaries-list1"><i class="fa fa-check"></i><b>1</b> Preliminaries {# list1}</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#exchangeability-and-de-finettis-theorem"><i class="fa fa-check"></i><b>1.1</b> Exchangeability and de Finetti’s theorem</a><ul>
<li class="chapter" data-level="1.1.1" data-path=""><a href="#de-finnetis-theorem"><i class="fa fa-check"></i><b>1.1.1</b> De Finneti’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#exponential-family-of-distributions"><i class="fa fa-check"></i><b>1.2</b> Exponential family of distributions</a><ul>
<li class="chapter" data-level="1.2.1" data-path=""><a href="#cumulants-and-moments-of-exponential-families"><i class="fa fa-check"></i><b>1.2.1</b> Cumulants and moments of exponential families</a></li>
<li class="chapter" data-level="1.2.2" data-path=""><a href="#conjugate-priors"><i class="fa fa-check"></i><b>1.2.2</b> Conjugate priors</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:title:end-->
<!--bookdown:title:start-->
<div id="preliminaries-list1" class="section level1">
<h1><span class="header-section-number">List 1</span> Preliminaries {# list1}</h1>
<div id="exchangeability-and-de-finettis-theorem" class="section level2">
<h2><span class="header-section-number">1.1</span> Exchangeability and de Finetti’s theorem</h2>
<p>A standard situation in statistics is to be presented with a sequence of observations, and use them to make predictions about future observations. In order to do so, we need to make certain assumptions about the nature of the statistical relationships between the sequence of observations.</p>
<p>A common assumption is that our data are <strong>exchangeable</strong>, meaning that their joint probability is invariant under permutations. More concretely, we say a sequence of N observations is finitely exchangeable if <span class="math display">\[P(X_1\in A_1,X_2\in A_2,\dots, X_N\in A_n) = P(X_{\sigma(1)}\in A_1, X_{\sigma(2)}\in A_2,\dots, X_{\sigma(N)}\in A_n)\]</span> for any permutation of the integers 1 through <span class="math inline">\(N\)</span>, and that an infinite sequence is infinitely exchangeable if this invariance holds for all values of <span class="math inline">\(N\)</span>.</p>

<div class="exercise">
<span id="exr:unnamed-chunk-1" class="exercise"><strong>Exercise 1.1  </strong></span>Clearly, all iid sequences are exchangeable, but not all exchangeable sequences are iid. Consider an urn, containing r red balls and b blue balls. A sequence of colors is generated by repeatedly sampling a ball from the urn, noting its color, and then returning the ball, plus another ball of the same color, to the urn. Show that the resulting sequence is exchangeable, but not iid.
</div>

<div id="de-finnetis-theorem" class="section level3">
<h3><span class="header-section-number">1.1.1</span> De Finneti’s theorem</h3>
<p>Loosely speaking, de Finetti’s Theorem states if a sequence of random variables is infinitely exchangeable, those random variables must be conditionally i.i.d. given some set of parameters. More formally,</p>

<div class="theorem">
<span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 1.1  (de Finetti)  </strong></span> Let <span class="math inline">\((X_1,X_2,\dots)\)</span> be an infinite sequence of random variables in some space <span class="math inline">\(\mathcal{X}\)</span>. This sequence is infinitely exchangeable if and only if there exists a probability distribution <span class="math inline">\(Q_\theta\)</span>, parametrized by some random parameter <span class="math inline">\(\theta\sim \nu\)</span>, such that the <span class="math inline">\(X_i\)</span> are conditionally iid given <span class="math inline">\(Q_\theta\)</span> and such that <span class="math display">\[P(X_1\in A_1,X_2\in A_2,\dots) = \int_{\Theta}\prod_{i=1}^\infty Q_\theta(A_i) \nu(d\theta).\]</span>
</div>

<p>This means we can imagine that any exchangeable sequence has been generated as a sequence of i.i.d. random variables with some unknown law. This provides a motivation for Bayesian inference: We have a hierarchical model, where data are generated according to some distribution parametrized by a random (in the Bayesian context – i.e. unknown/uncertain) variable <span class="math inline">\(\theta\)</span>, and our uncertainty about <span class="math inline">\(\theta\)</span> is characterized by some distribution <span class="math inline">\(\nu\)</span>.</p>
<p>Let’s consider the 0/1 form of de Finetti’s theorem, for exchangeable sequences of binary variables:</p>

<div class="theorem">
<span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 1.2  (de Finetti 0/1)  </strong></span> An infinite sequence <span class="math inline">\((X_1,X_2,\dots)\)</span> of binary random variables is exchangeable if and only if its distribution can be written as <span class="math display">\[\begin{aligned}P(X_1=x_1,X_2=x_2,\dots, X_N=x_N)  =&amp; \int_0^1\prod_{i=1}^N\left\{\theta^{x_i}(1-\theta)^{1-x_i}\right\} d\nu(\theta)\\
    =&amp;\int_0^1 \theta^{k}(1-\theta)^{N-k} d\nu(\theta)\end{aligned}\]</span> where <span class="math inline">\(k=\sum_ix_i\)</span>.
</div>

<p>We will now work through (most of) a proof in the next two exercises.</p>

<div class="exercise">
<span id="exr:unnamed-chunk-4" class="exercise"><strong>Exercise 1.2  </strong></span> We will start off with a finite sequence <span class="math inline">\((X_1,\dots, X_M)\)</span>. For any <span class="math inline">\(N\leq M\)</span>, show that <span class="math display">\[P\left(\sum_{i=1}^N X_i = s\Big|\sum_{i=1}^M X_i = t\right) = \frac{{t\choose s}{M-t\choose N-s}}{{M \choose N}}\]</span>
</div>

We can therefore write
<span class="math display">\[\begin{equation}
P\left(\sum_{i=1}^N X_i = s\right) = {N\choose s}\sum_{t=s}^{M-N+s}\frac{(t)_s(M-t)_{n-s}}{(M)_N}P\left(\sum_{i=1}^M X_i = t\right),
\label{eqn:a}
\end{equation}\]</span>
<p>where <span class="math inline">\((x)_y = x(x-1)\dots (x-y+1)\)</span>.</p>
<p>Let <span class="math inline">\(F_M(\theta)\)</span> be the distribution function of <span class="math inline">\(\frac{1}{M}(X_1, + \dots, + X_M)\)</span> – i.e. a step function between 0 and 1, with steps of size <span class="math inline">\(P(\sum_i X_i= t)\)</span> at <span class="math inline">\(t=0,1,\dots, M\)</span>. Then we can rewrite Equation~ as</p>
<span class="math display">\[P\left(\sum_{i=1}^N X_i = s\right) = {N\choose s} \int_0^1\frac{(M\theta)_s(M(1-\theta))_{N-s}}{(M)_N}dF_M(\theta)\]</span> 
<div class="exercise">
<span id="exr:unnamed-chunk-5" class="exercise"><strong>Exercise 1.3  </strong></span> Show that, as <span class="math inline">\(M\rightarrow \infty\)</span>, we can write <span class="math display">\[P\left(\sum_{i=1}^N X_i = s\right) \rightarrow {N\choose s} \int_0^1\theta^s(1-\theta)^{N-s}dF_M(\theta)\]</span>
</div>

<p>The proof is completed using a result (the Helly Theorem), that shows that any sequence <span class="math inline">\(\{F_M(\theta); M=1,2,\dot\}\)</span> of probability distributions on [0,1] contains a subsequence that converges to <span class="math inline">\(F(\theta)\)</span>.</p>
</div>
</div>
<div id="exponential-family-of-distributions" class="section level2">
<h2><span class="header-section-number">1.2</span> Exponential family of distributions</h2>
<p>De Finetti’s theorem can be seen as a motivation for Bayesian inference. If our data are exchangeable, we know that they are iid according to some unknown probability distribution <span class="math inline">\(F_\theta(X)\)</span>, which we can think of as a <strong>likelihood function</strong>, and that they can be represented using an mixture of such iid sequences. As we saw from the 0/1 case, the distribution over probabilities is given by the limit of the empirical distribution function. When not working in this limit, we may choose to model this distribution over the parameters of our likelihood function using a <strong>prior</strong> distribution <span class="math inline">\(\pi(\theta)\)</span> – ideally one that both assigns probability mass to where we expect the empirical distribution might concentrate, and for which <span class="math inline">\(\int_\Theta F_\theta(X) \pi(d\theta)\)</span> is tractable.</p>
<p>The exponential family of probability distributions is the class of distributions parametrized by <span class="math inline">\(\theta\)</span> whose density can be written as</p>
<p><span class="math display">\[p(x|\theta) = h(x)\exp\{\eta(\theta)^TT(x) - A(\eta(\theta))\}\]</span> where</p>
<ul>
<li><span class="math inline">\(\eta(\theta)\)</span> (sometimes just written as <span class="math inline">\(\eta\)</span>), is a transformation of <span class="math inline">\(\theta\)</span> that is often referred to as the .</li>
<li><span class="math inline">\(T(X)\)</span> is known as a  of <span class="math inline">\(X\)</span>. We see that <span class="math inline">\(p(x|\theta)\)</span> depends only on <span class="math inline">\(X\)</span> through <span class="math inline">\(T(X)\)</span>, implying that <span class="math inline">\(T(X)\)</span> contains all the relevant information about <span class="math inline">\(X\)</span>. <em>\item</em> <span class="math inline">\(A(\eta(\theta)\)</span> (or <span class="math inline">\(A(\eta)\)</span>) is known as the  or the  (remember, a partition function provides a normalizing constant).</li>
</ul>

<div class="example">
<p><span id="exm:unnamed-chunk-6" class="example"><strong>Example 1.1  (The Bernoulli distribution)  </strong></span> A Bernoulli random variable <span class="math inline">\(X\)</span> takes the value <span class="math inline">\(X=1\)</span> with probability <span class="math inline">\(\pi\)</span> and <span class="math inline">\(X=0\)</span> with probability <span class="math inline">\(1-\pi\)</span>; it’s density can be written:</p>
<span class="math display">\[\begin{aligned}
    p(x|\pi) =&amp; \pi^x(1-\pi)^{1-x}\\
    =&amp; \exp\left\{\log\left(\frac{\pi}{1-\pi}\right)x + \log(1-\pi)\right\}\end{aligned}\]</span> By rewriting in this exponential family form, we see that

</div>


<div class="exercise">
<span id="exr:unnamed-chunk-7" class="exercise"><strong>Exercise 1.4  </strong></span> The Poisson random variable has PDF <span class="math display">\[p(x|\lambda) = \frac{\lambda^xe^{-\lambda}}{x!}\]</span> Re-write the density of the Poisson random variable in exponential family form. What are <span class="math inline">\(\eta\)</span>, <span class="math inline">\(T(x)\)</span>, <span class="math inline">\(A(\eta)\)</span> and <span class="math inline">\(h(x)\)</span>? What about if we have <span class="math inline">\(n\)</span> independent samples <span class="math inline">\(x_1,\dots, x_n\)</span>?
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-8" class="exercise"><strong>Exercise 1.5  </strong></span> The gamma random variable has PDF <span class="math display">\[p(x|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\]</span> What are the natural parameters and sufficient statistics for the gamma distribution, given <span class="math inline">\(n\)</span> observations <span class="math inline">\(x_1,\dots, x_N\)</span>?
</div>

<div id="cumulants-and-moments-of-exponential-families" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Cumulants and moments of exponential families</h3>
<p>We are probably most familiar with using the PDF or the CDF of a random variable to describe its distribution, but there are other representations that can be useful. The  <span class="math inline">\(M_X(s) = \E[\exp(s^Tx)] = \int_\mathcal{X} e^{s^Tx}p_X(x) dx\)</span> is the Laplace transform of the PDF <span class="math inline">\(p_X(x)\)</span>. As the name suggests, we can use the moment-generating function to generate the (uncentered) moments of a random variable; the <span class="math inline">\(n\)</span>th moment is given by</p>
<p><span class="math display">\[m_n = \frac{d^nM_X}{ds^n}\Bigr|_{s=0}\]</span></p>

<div class="exercise">
<span id="exr:unnamed-chunk-9" class="exercise"><strong>Exercise 1.6  </strong></span> For exponential family random variables, we know that the sufficient statistic <span class="math inline">\(T(X)\)</span> contains all the information about <span class="math inline">\(X\)</span>, so (for univariate <span class="math inline">\(X\)</span>) we can write the moment generating function of the sufficient statistic as <span class="math inline">\(\E[e^{sT(x)}|\eta]\)</span>. Show that the moment generating function for the sufficient statistic of an arbitrary exponential family random variable with natural parameter <span class="math inline">\(\eta\)</span> can be written as <span class="math display">\[M_{T(X)}(s) = \exp{A(\eta+s) - A(\eta)}\]</span>
</div>

<p>A related representation is the  <span class="math inline">\(C_X(s) = \log \E[e^{s^Tx}] = \log(M_X(s))\)</span>. Clearly, for exponential families this takes the form <span class="math inline">\(C_{T(X)}(s) = A(\eta+s)-A(\eta)\)</span>. This explains why <span class="math inline">\(A(\eta)\)</span> is sometimes called the cumulant function! The cumulant function can be used to generate the cumulants of a distribution as</p>
<p><span class="math display">\[\kappa_n = \frac{d^nC_X}{ds^n}\Bigr|_{s=0}\]</span></p>
<p>The first three cumulants are the same as the first three central moments of the distribution – meaning, the cumulant generative function is a useful tool for calculating mean, variance and the third central moment.</p>

<div class="exercise">
<span id="exr:unnamed-chunk-10" class="exercise"><strong>Exercise 1.7  </strong></span> It is usually easier to calculate mean and variance using the cumulant generating function rather than the moment generating function. Starting from the exponential family representation of the Poisson distribution from Exercise 1.4, calculate the mean and variance of the Poisson using a) the moment generating function, and b) the cumulant generating function.
</div>

</div>
<div id="conjugate-priors" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Conjugate priors</h3>
<p>Exponential families are very important in Bayesian statistics because, for any exponential family likelihood, we can find an conjugate exponential family prior. If our likelihood takes the form</p>
<p><span class="math display">\[f(x|\eta) = h(x)\exp\left\{\eta^TT(x) - A(\eta)\right\}\]</span></p>
<p>then a conjugate prior is given by</p>
<p><span class="math display">\[p(\eta|\xi,\nu) = g(\xi,\nu)\exp\left\{\eta^T\xi - \nu A(\eta)\right\}\]</span></p>
<p>Below are some exercises based on common conjugate priors.</p>

<div class="exercise">
<span id="exr:unnamed-chunk-11" class="exercise"><strong>Exercise 1.8  </strong></span> Suppose we have <span class="math inline">\(N\)</span> independent observations <span class="math inline">\(x_1,\dots, x_N \stackrel{\small{iid}}{\sim}\mbox{Normal}(\mu, \sigma^2)\)</span>. If <span class="math inline">\(\sigma^2\)</span> is known and <span class="math inline">\(\mu\sim \mbox{Normal}(\mu_0,\sigma_0^2)\)</span>, derive the posterior for <span class="math inline">\(\mu|x_1,\dots, x_N\)</span>
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-12" class="exercise"><strong>Exercise 1.9  </strong></span> Now, let’s assume <span class="math inline">\(x_1,\dots, x_N \stackrel{\small{iid}}{\sim}\mbox{Normal}(\mu, \sigma^2)\)</span> with known mean <span class="math inline">\(\mu\)</span> but unknown variance <span class="math inline">\(\sigma^2\)</span>. Let’s express the likelihood in terms of the precision, <span class="math inline">\(\omega=1\sigma^2\)</span>: <span class="math display">\[f(x_i|\mu, \omega) = \sqrt{\frac{\omega}{2\pi}} \exp\left\{-\frac{\omega}{2}(x_i-\mu)^2\right\}\]</span> Let <span class="math inline">\(\omega\)</span> have a gamma prior (this is also known as putting an inverse-gamma prior on <span class="math inline">\(\sigma^2\)</span>): <span class="math display">\[p(\omega) = \frac{\beta^\alpha}{\Gamma(\alpha)}\omega^{\alpha-1}e^{-\beta \omega}\]</span> Derive the posterior distribution for <span class="math inline">\(\omega\)</span>
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-13" class="exercise"><strong>Exercise 1.10  </strong></span> Let’s assume <span class="math inline">\(x \sim \mbox{Normal}(0, \sigma^2)\)</span> and that <span class="math inline">\(\sigma^2\sim\mbox{InvGamma}(\alpha,\beta)\)</span> (i.e. <span class="math inline">\(1/\sigma^2 \sim \mbox{Gamma}(\alpha,\beta)\)</span>). Show that the marginal distribution of <span class="math inline">\(x\)</span> is given by a Student’s <span class="math inline">\(t\)</span> distribution.
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
